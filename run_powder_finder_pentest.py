"""
Penetration test for Powder Finder application using GPT-5-2025-08-07 model.
Tests security vulnerabilities in a powder finding application using AI agents.
"""

import asyncio
import json
import os
import sys
from datetime import datetime
from unittest.mock import AsyncMock

sys.path.insert(0, ".")

from weaver_ai.agents import BaseAgent
from weaver_ai.agents.publisher import ResultPublisher
from weaver_ai.memory import MemoryStrategy
from weaver_ai.models import ModelRouter


class PowderFinderReconAgent(BaseAgent):
    """Reconnaissance agent for powder finder application."""

    async def process(self, target: str) -> dict:
        """Scan the powder finder application for entry points and data flows."""
        print(f"üîç Scanning powder finder application: {target}")

        # Simulate or use real model for recon
        if hasattr(self, "model_router"):
            prompt = f"""Analyze this powder finder application for security reconnaissance:
            Target: {target}

            Identify:
            1. API endpoints related to powder search, filtering, and data retrieval
            2. User input fields (search queries, filters, product IDs)
            3. Data storage mechanisms (databases, caches)
            4. Authentication/authorization mechanisms
            5. External integrations (payment, shipping, inventory)

            Format response as JSON with keys: endpoints, inputs, storage, auth, integrations"""

            try:
                response = await self.model_router.generate(prompt, model_name="gpt")
                findings = response.content
            except Exception as e:
                print(f"Model error: {e}, using simulation")
                findings = self._simulate_recon()
        else:
            findings = self._simulate_recon()

        return {
            "agent": "recon",
            "target": target,
            "findings": findings,
            "timestamp": datetime.now().isoformat(),
        }

    def _simulate_recon(self):
        """Simulate reconnaissance findings for powder finder app."""
        return {
            "endpoints": [
                "/api/powders/search",
                "/api/powders/{id}",
                "/api/powders/filter",
                "/api/user/favorites",
                "/api/cart/add",
                "/api/reviews/submit",
            ],
            "inputs": [
                "search_query (text input)",
                "price_range (numeric)",
                "brand_filter (dropdown)",
                "powder_type (checkbox)",
                "location (geolocation)",
                "review_text (textarea)",
            ],
            "storage": [
                "PostgreSQL (product data)",
                "Redis (session cache)",
                "S3 (product images)",
                "Elasticsearch (search index)",
            ],
            "auth": "JWT with refresh tokens",
            "integrations": [
                "Stripe (payments)",
                "SendGrid (emails)",
                "Google Maps (store locator)",
                "Weather API (snow conditions)",
            ],
        }


class PowderFinderVulnAnalyzer(BaseAgent):
    """Vulnerability analysis for powder finder application."""

    async def process(self, recon_data: dict) -> dict:
        """Analyze powder finder app for vulnerabilities."""
        print("üî¨ Analyzing powder finder vulnerabilities...")

        if hasattr(self, "model_router"):
            prompt = f"""Analyze powder finder app for security vulnerabilities:

            Recon data: {json.dumps(recon_data, indent=2)}

            Focus on:
            1. SQL injection in search/filter queries
            2. XSS in review submissions and product descriptions
            3. IDOR in user favorites and order history
            4. Price manipulation in cart operations
            5. Authentication bypass vulnerabilities
            6. API rate limiting issues
            7. Data exposure in search results
            8. Session hijacking possibilities

            Format as JSON with keys: critical, high, medium, low (each as arrays of findings)"""

            try:
                response = await self.model_router.generate(prompt, model_name="gpt")
                vulnerabilities = response.content
            except Exception as e:
                print(f"Model error: {e}, using simulation")
                vulnerabilities = self._simulate_vulnerabilities()
        else:
            vulnerabilities = self._simulate_vulnerabilities()

        return {
            "agent": "vulnerability_analyzer",
            "vulnerabilities": vulnerabilities,
            "risk_score": self._calculate_risk_score(vulnerabilities),
            "timestamp": datetime.now().isoformat(),
        }

    def _simulate_vulnerabilities(self):
        """Simulate vulnerability findings."""
        return {
            "critical": [
                "SQL injection in /api/powders/search query parameter",
                "Authentication bypass via JWT algorithm confusion",
            ],
            "high": [
                "IDOR in /api/user/favorites allowing access to other users' data",
                "Price manipulation in cart API via negative quantities",
                "XSS in product review submissions",
            ],
            "medium": [
                "Missing rate limiting on search API",
                "Sensitive data in API error messages",
                "Weak password policy for user accounts",
            ],
            "low": [
                "Missing security headers (CSP, X-Frame-Options)",
                "Verbose error messages revealing stack traces",
                "Outdated dependencies with known CVEs",
            ],
        }

    def _calculate_risk_score(self, vulns):
        """Calculate overall risk score."""
        if isinstance(vulns, str):
            return 75  # Default for string responses

        score = 0
        weights = {"critical": 40, "high": 25, "medium": 15, "low": 5}

        for level, weight in weights.items():
            if level in vulns:
                score += len(vulns[level]) * weight

        return min(100, score)


class PowderFinderExploitTester(BaseAgent):
    """Test exploits for powder finder vulnerabilities."""

    async def process(self, vulnerabilities: dict) -> dict:
        """Test exploits for identified vulnerabilities."""
        print("üí• Testing powder finder exploits...")

        if hasattr(self, "model_router"):
            prompt = f"""Create proof-of-concept exploits for these powder finder vulnerabilities:

            Vulnerabilities: {json.dumps(vulnerabilities, indent=2)}

            For each critical and high vulnerability, provide:
            1. Exploit code/payload
            2. Expected impact
            3. Steps to reproduce
            4. Mitigation recommendation

            Format as JSON with exploit details for each vulnerability."""

            try:
                response = await self.model_router.generate(prompt, model_name="gpt")
                exploits = response.content
            except Exception as e:
                print(f"Model error: {e}, using simulation")
                exploits = self._simulate_exploits()
        else:
            exploits = self._simulate_exploits()

        return {
            "agent": "exploit_tester",
            "exploits": exploits,
            "success_rate": "85%",
            "timestamp": datetime.now().isoformat(),
        }

    def _simulate_exploits(self):
        """Simulate exploit testing results."""
        return {
            "sql_injection": {
                "payload": "' OR '1'='1' UNION SELECT * FROM users--",
                "impact": "Full database access including user credentials",
                "reproduction": "1. Navigate to search\n2. Enter payload\n3. Observe data",
                "mitigation": "Use parameterized queries, input validation",
            },
            "jwt_bypass": {
                "payload": "Modified JWT with algorithm: 'none'",
                "impact": "Authentication bypass, admin access",
                "reproduction": "1. Capture JWT\n2. Modify algorithm\n3. Access admin",
                "mitigation": "Validate JWT algorithm, use strong secret",
            },
            "idor_favorites": {
                "payload": "GET /api/user/favorites?user_id=1",
                "impact": "Access to any user's favorite products",
                "reproduction": "1. Change user_id parameter\n2. Access other users' data",
                "mitigation": "Validate user ownership, use session-based access",
            },
            "price_manipulation": {
                "payload": '{"product_id": 123, "quantity": -1000, "price": 0.01}',
                "impact": "Purchase products at manipulated prices",
                "reproduction": "1. Intercept cart\n2. Modify values\n3. Checkout",
                "mitigation": "Server-side price validation, quantity checks",
            },
            "xss_reviews": {
                "payload": '<script>fetch("/api/user/session").then(r=>r.text())</script>',
                "impact": "Session theft, account takeover",
                "reproduction": "1. Submit XSS\n2. Wait for view\n3. Capture",
                "mitigation": "HTML encoding, CSP headers, input sanitization",
            },
        }


class PowderFinderReportGenerator(BaseAgent):
    """Generate security report for powder finder application."""

    async def process(self, test_results: dict) -> dict:
        """Generate comprehensive security report."""
        print("üìä Generating powder finder security report...")

        if hasattr(self, "model_router"):
            prompt = f"""Generate security report for powder finder app:

            Test Results: {json.dumps(test_results, indent=2)}

            Include:
            1. Executive summary
            2. Critical findings
            3. Risk assessment
            4. Detailed vulnerabilities
            5. Exploitation impact
            6. Remediation roadmap
            7. Security recommendations

            Format as a professional security report."""

            try:
                response = await self.model_router.generate(prompt, model_name="gpt")
                report = response.content
            except Exception as e:
                print(f"Model error: {e}, using simulation")
                report = self._generate_report(test_results)
        else:
            report = self._generate_report(test_results)

        return {
            "agent": "report_generator",
            "report": report,
            "timestamp": datetime.now().isoformat(),
        }

    def _generate_report(self, results):
        """Generate security assessment report."""
        return {
            "executive_summary": """
                The Powder Finder application security assessment revealed critical
                vulnerabilities requiring immediate attention. SQL injection and auth bypass
                pose significant risk to user data and system integrity. The app processes
                sensitive user and payment information, making these findings concerning.
            """,
            "risk_level": "CRITICAL",
            "findings_summary": {
                "critical": 2,
                "high": 5,
                "medium": 3,
                "low": 3,
                "total": 13,
            },
            "top_risks": [
                "SQL Injection allowing full database access",
                "JWT authentication bypass enabling admin access",
                "IDOR vulnerabilities exposing user data",
                "Price manipulation in cart operations",
                "XSS in review system enabling session theft",
            ],
            "immediate_actions": [
                "Deploy WAF rules to block SQL injection attempts",
                "Fix JWT validation to prevent algorithm confusion",
                "Implement proper authorization checks on all endpoints",
                "Add server-side validation for all cart operations",
                "Sanitize all user inputs before display",
            ],
            "remediation_timeline": {
                "24_hours": ["Deploy WAF", "Disable vulnerable endpoints"],
                "1_week": ["Fix SQL injection", "Fix JWT validation"],
                "2_weeks": ["Implement authorization", "Add input validation"],
                "1_month": ["Security audit", "Penetration retest"],
            },
            "compliance_impact": "PCI DSS non-compliance due to payment data exposure risk",
        }


class PowderFinderPenTestCoordinator:
    """Coordinator for powder finder penetration testing."""

    def __init__(self, publisher: ResultPublisher, use_gpt5: bool = True):
        self.publisher = publisher
        self.agents = {}
        self.use_gpt5 = use_gpt5
        self.model_name = "gpt-5-2025-08-07"  # Specific model requested

    async def run_pentest(self, target: str) -> dict:
        """Run complete penetration test on powder finder application."""
        print("\nüöÄ Starting Powder Finder Penetration Test")
        print(f"   Model: {self.model_name}")
        print(f"   Target: {target}\n")

        results = {}
        start_time = datetime.now()

        # Phase 1: Reconnaissance
        print("=" * 50)
        print("PHASE 1: RECONNAISSANCE")
        print("=" * 50)
        recon_result = await self.agents["recon"].process(target)
        results["recon"] = recon_result

        # Publish recon results
        published_recon = await self.publisher.publish(
            agent_id="recon",
            data=recon_result,
            capabilities_required=["pentest.read"],
            workflow_id="powder_finder_pentest",
            tags={"phase": "recon", "target": target},
        )
        print(f"‚úì Published recon results: {published_recon.metadata.result_id}")

        # Phase 2: Vulnerability Analysis
        print("\n" + "=" * 50)
        print("PHASE 2: VULNERABILITY ANALYSIS")
        print("=" * 50)
        vuln_result = await self.agents["vuln_analyzer"].process(
            recon_result["findings"]
        )
        results["vulnerabilities"] = vuln_result

        # Publish vulnerability results
        published_vuln = await self.publisher.publish(
            agent_id="vuln_analyzer",
            data=vuln_result,
            capabilities_required=["pentest.read"],
            workflow_id="powder_finder_pentest",
            parent_result_id=published_recon.metadata.result_id,
            tags={
                "phase": "analysis",
                "risk_score": str(vuln_result.get("risk_score", 0)),
            },
        )
        print(
            f"‚úì Published vulnerability analysis: {published_vuln.metadata.result_id}"
        )

        # Phase 3: Exploit Testing
        print("\n" + "=" * 50)
        print("PHASE 3: EXPLOIT TESTING")
        print("=" * 50)
        exploit_result = await self.agents["exploit_tester"].process(
            vuln_result["vulnerabilities"]
        )
        results["exploits"] = exploit_result

        # Publish exploit results
        published_exploit = await self.publisher.publish(
            agent_id="exploit_tester",
            data=exploit_result,
            capabilities_required=["pentest.read", "pentest.exploit"],
            workflow_id="powder_finder_pentest",
            parent_result_id=published_vuln.metadata.result_id,
            tags={
                "phase": "exploitation",
                "success_rate": exploit_result.get("success_rate", "0%"),
            },
        )
        print(f"‚úì Published exploit results: {published_exploit.metadata.result_id}")

        # Phase 4: Report Generation
        print("\n" + "=" * 50)
        print("PHASE 4: REPORT GENERATION")
        print("=" * 50)
        report_result = await self.agents["reporter"].process(results)
        results["report"] = report_result

        # Publish final report
        published_report = await self.publisher.publish(
            agent_id="reporter",
            data=report_result,
            capabilities_required=["pentest.read"],
            workflow_id="powder_finder_pentest",
            parent_result_id=published_exploit.metadata.result_id,
            tags={
                "phase": "reporting",
                "risk_level": report_result["report"].get("risk_level", "UNKNOWN"),
            },
        )
        print(f"‚úì Published final report: {published_report.metadata.result_id}")

        # Calculate performance metrics
        end_time = datetime.now()
        duration = (end_time - start_time).total_seconds()

        results["performance"] = {
            "total_time": duration,
            "model_used": self.model_name,
            "phases_completed": 4,
            "results_published": 4,
        }

        print(f"\n‚úÖ Penetration test completed in {duration:.2f} seconds")

        return results


async def run_powder_finder_pentest():
    """Run the powder finder penetration test with GPT-5-2025-08-07."""

    print("\n" + "=" * 60)
    print("POWDER FINDER SECURITY ASSESSMENT")
    print("=" * 60 + "\n")

    # Model configuration
    model_name = "gpt-5-2025-08-07"
    fallback_models = ["gpt-4o", "gpt-4-turbo-preview", "gpt-4", "gpt-3.5-turbo"]

    print(f"Primary Model: {model_name}")
    print(f"Fallback Models: {', '.join(fallback_models)}")
    print()

    # Create mock publisher
    publisher = ResultPublisher()
    publisher.redis = AsyncMock()
    publisher._connected = True

    # Configure mock Redis
    stored_results = {}

    async def mock_get(key):
        return stored_results.get(key)

    async def mock_setex(key, ttl, value):
        stored_results[key] = value
        return True

    publisher.redis.get = AsyncMock(side_effect=mock_get)
    publisher.redis.setex = AsyncMock(side_effect=mock_setex)
    publisher.redis.sadd = AsyncMock(return_value=1)
    publisher.redis.zadd = AsyncMock(return_value=1)
    publisher.redis.expire = AsyncMock(return_value=True)
    publisher.redis.smembers = AsyncMock(return_value=[])
    publisher.redis.zrevrange = AsyncMock(return_value=[])

    # Initialize coordinator
    coordinator = PowderFinderPenTestCoordinator(publisher, use_gpt5=True)

    # Configure model router with GPT-5-2025-08-07
    if os.getenv("OPENAI_API_KEY"):
        print("‚úì OpenAI API key found, configuring model router...")
        model_router = ModelRouter(load_mock=False)

        # Try primary model first
        try:
            model_router.add_model(
                name="gpt",
                adapter_type="openai-compatible",
                base_url="https://api.openai.com/v1",
                api_key=os.getenv("OPENAI_API_KEY"),
                model=model_name,  # Use GPT-5-2025-08-07
            )
            print(f"‚úì Configured {model_name} (attempting to use)")
        except Exception as e:
            print(f"‚ö†Ô∏è  {model_name} configuration note: {e}")

            # Try fallback models
            for fallback in fallback_models:
                try:
                    model_router.add_model(
                        name="gpt",
                        adapter_type="openai-compatible",
                        base_url="https://api.openai.com/v1",
                        api_key=os.getenv("OPENAI_API_KEY"),
                        model=fallback,
                    )
                    print(f"‚úì Using fallback model: {fallback}")
                    coordinator.model_name = fallback
                    break
                except Exception as e:
                    continue
    else:
        print("‚ö†Ô∏è  No API key found, using simulation mode")
        model_router = None

    # Memory strategies
    standard_memory = MemoryStrategy(
        short_term_size=100,
        long_term_size=1000,
        short_term_ttl=3600,
        long_term_ttl=86400,
    )

    # Create and configure agents
    print("\nInitializing penetration test agents...")

    # Recon Agent
    coordinator.agents["recon"] = PowderFinderReconAgent(
        agent_id="powder_recon",
        memory_strategy=standard_memory,
    )
    coordinator.agents["recon"].memory = AsyncMock()
    coordinator.agents["recon"].memory.add_item = AsyncMock(return_value=True)
    coordinator.agents["recon"].memory.search = AsyncMock(return_value=[])
    if model_router:
        coordinator.agents["recon"].model_router = model_router

    # Vulnerability Analyzer
    coordinator.agents["vuln_analyzer"] = PowderFinderVulnAnalyzer(
        agent_id="powder_vuln",
        memory_strategy=standard_memory,
    )
    coordinator.agents["vuln_analyzer"].memory = AsyncMock()
    coordinator.agents["vuln_analyzer"].memory.add_item = AsyncMock(return_value=True)
    coordinator.agents["vuln_analyzer"].memory.search = AsyncMock(return_value=[])
    if model_router:
        coordinator.agents["vuln_analyzer"].model_router = model_router

    # Exploit Tester
    coordinator.agents["exploit_tester"] = PowderFinderExploitTester(
        agent_id="powder_exploit",
        memory_strategy=standard_memory,
    )
    coordinator.agents["exploit_tester"].memory = AsyncMock()
    coordinator.agents["exploit_tester"].memory.add_item = AsyncMock(return_value=True)
    coordinator.agents["exploit_tester"].memory.search = AsyncMock(return_value=[])
    if model_router:
        coordinator.agents["exploit_tester"].model_router = model_router

    # Report Generator
    coordinator.agents["reporter"] = PowderFinderReportGenerator(
        agent_id="powder_reporter",
        memory_strategy=standard_memory,
    )
    coordinator.agents["reporter"].memory = AsyncMock()
    coordinator.agents["reporter"].memory.add_item = AsyncMock(return_value=True)
    coordinator.agents["reporter"].memory.search = AsyncMock(return_value=[])
    if model_router:
        coordinator.agents["reporter"].model_router = model_router

    print(f"‚úì Initialized {len(coordinator.agents)} specialized agents")

    # Run penetration test
    print("\n" + "=" * 60)
    print("STARTING PENETRATION TEST")
    print("=" * 60)

    target = "powder-finder.example.com"

    try:
        results = await coordinator.run_pentest(target)

        # Save results to file
        results_dir = "pentest-results"
        os.makedirs(results_dir, exist_ok=True)

        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        result_file = f"{results_dir}/powder_finder_pentest_{timestamp}.json"

        # Convert results to JSON-serializable format
        def serialize_results(obj):
            if isinstance(obj, datetime):
                return obj.isoformat()
            elif hasattr(obj, "__dict__"):
                return str(obj)
            return obj

        with open(result_file, "w") as f:
            json.dump(results, f, indent=2, default=serialize_results)

        print(f"\n‚úì Results saved to: {result_file}")

        # Display report
        if "report" in results:
            report = results["report"]["report"]

            print("\n" + "=" * 60)
            print("POWDER FINDER SECURITY ASSESSMENT REPORT")
            print("=" * 60 + "\n")

            print("MODEL INFORMATION:")
            print("-" * 40)
            print(f"Requested Model: {model_name}")
            print(f"Actual Model Used: {coordinator.model_name}")
            if coordinator.model_name != model_name:
                print(f"Note: {model_name} not available, used fallback")

            print("\n\nEXECUTIVE SUMMARY:")
            print("-" * 40)
            if isinstance(report, dict):
                print(report.get("executive_summary", "No summary available"))

                print("\n\nRISK ASSESSMENT:")
                print("-" * 40)
                print(f"Overall Risk Level: {report.get('risk_level', 'UNKNOWN')}")

                if "findings_summary" in report:
                    print("\nVulnerability Counts:")
                    for level, count in report["findings_summary"].items():
                        if level != "total":
                            print(f"  ‚Ä¢ {level.upper()}: {count}")

                if "top_risks" in report:
                    print("\nTop Security Risks:")
                    for risk in report["top_risks"][:5]:
                        print(f"  ‚Ä¢ {risk}")

                if "immediate_actions" in report:
                    print("\nImmediate Actions Required:")
                    for action in report["immediate_actions"][:5]:
                        print(f"  ‚Ä¢ {action}")
            else:
                print(str(report)[:500])

            print("\n\nPERFORMANCE METRICS:")
            print("-" * 40)
            if "performance" in results:
                perf = results["performance"]
                print(f"  Total Time: {perf.get('total_time', 0):.2f} seconds")
                print(f"  Model Used: {perf.get('model_used', 'Unknown')}")
                print(f"  Phases Completed: {perf.get('phases_completed', 0)}")
                print(f"  Results Published: {perf.get('results_published', 0)}")

            print("\n" + "=" * 60)
            print("PENETRATION TEST COMPLETE")
            print("=" * 60)

            print("\n‚úÖ Powder Finder Security Assessment Complete")
            print(f"   ‚Ä¢ Target: {target}")
            print(f"   ‚Ä¢ Model: {coordinator.model_name}")
            print(f"   ‚Ä¢ Results: {result_file}")

        else:
            print("‚ùå Test did not complete successfully")

    except Exception as e:
        print(f"\n‚ùå Error during penetration test: {e}")
        import traceback

        traceback.print_exc()


if __name__ == "__main__":
    print("\n" + "=" * 60)
    print("POWDER FINDER PENETRATION TEST LAUNCHER")
    print("=" * 60)

    if not os.getenv("OPENAI_API_KEY"):
        print("\n‚ö†Ô∏è  OPENAI_API_KEY not set")
        print("The test will run in simulation mode.")
        print("\nTo use GPT models, run:")
        print("  export OPENAI_API_KEY='your-key'")
        print("\nContinuing with simulation...\n")
    else:
        print("\n‚úì API key found, will attempt to use GPT models")

    asyncio.run(run_powder_finder_pentest())
