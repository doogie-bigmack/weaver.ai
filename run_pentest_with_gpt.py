"""Run the full penetration testing workflow with real GPT models."""

import asyncio
import os

# Set up path
import sys
from unittest.mock import AsyncMock

sys.path.insert(0, ".")

# Import our test agents
from tests.integration.test_full_pentest_workflow import (
    ExploitTester,
    PenTestCoordinator,
    ReconAgent,
    ReportGenerator,
    VulnerabilityAnalyzer,
)
from weaver_ai.agents.publisher import ResultPublisher
from weaver_ai.memory import MemoryStrategy
from weaver_ai.models import ModelRouter


async def run_with_gpt():
    """Run the penetration test with real GPT models."""

    print("\n" + "=" * 60)
    print("SETTING UP GPT-POWERED PENETRATION TEST")
    print("=" * 60 + "\n")

    # Create mock publisher (no Redis needed)
    publisher = ResultPublisher()
    publisher.redis = AsyncMock()
    publisher._connected = True

    # Configure mock Redis operations
    publisher.redis.setex = AsyncMock(return_value=True)
    publisher.redis.sadd = AsyncMock(return_value=1)
    publisher.redis.zadd = AsyncMock(return_value=1)
    publisher.redis.expire = AsyncMock(return_value=True)
    publisher.redis.smembers = AsyncMock(return_value=[])
    publisher.redis.zrevrange = AsyncMock(return_value=[])

    # Store for retrieval simulation
    stored_results = {}

    async def mock_get(key):
        return stored_results.get(key)

    async def mock_setex(key, ttl, value):
        stored_results[key] = value
        return True

    publisher.redis.get = AsyncMock(side_effect=mock_get)
    publisher.redis.setex = AsyncMock(side_effect=mock_setex)

    # Create coordinator with GPT model
    print("Configuring GPT model...")
    coordinator = PenTestCoordinator(publisher, use_model=True)

    # Override the setup to use GPT
    async def setup_agents_with_gpt():
        # Create model router with GPT
        model_router = ModelRouter(load_mock=False)

        # Add GPT-4 (most advanced available model)
        model_router.add_model(
            name="gpt",
            adapter_type="openai-compatible",
            base_url="https://api.openai.com/v1",
            api_key=os.getenv("OPENAI_API_KEY"),
            model="gpt-4-turbo-preview",  # Using GPT-4 for best performance
        )

        print("✓ Configured GPT-4-turbo-preview model")

        # Memory strategies
        recon_memory = MemoryStrategy(
            short_term_size=100,
            long_term_size=1000,
            short_term_ttl=1800,
            long_term_ttl=86400,
        )

        analysis_memory = MemoryStrategy(
            short_term_size=200,
            long_term_size=2000,
            short_term_ttl=3600,
            long_term_ttl=172800,
        )

        # Create agents with mock memory backend
        coordinator.agents["recon"] = ReconAgent(
            agent_id="recon_gpt",
            memory_strategy=recon_memory,
        )

        # Mock the memory initialization to avoid Redis
        coordinator.agents["recon"].memory = AsyncMock()
        coordinator.agents["recon"].memory.add_item = AsyncMock(return_value=True)
        coordinator.agents["recon"].memory.search = AsyncMock(return_value=[])
        coordinator.agents["recon"].model_router = model_router

        coordinator.agents["vuln_analyzer"] = VulnerabilityAnalyzer(
            agent_id="vuln_gpt",
            memory_strategy=analysis_memory,
        )
        coordinator.agents["vuln_analyzer"].memory = AsyncMock()
        coordinator.agents["vuln_analyzer"].memory.add_item = AsyncMock(
            return_value=True
        )
        coordinator.agents["vuln_analyzer"].memory.search = AsyncMock(return_value=[])
        coordinator.agents["vuln_analyzer"].model_router = model_router

        coordinator.agents["exploit_tester"] = ExploitTester(
            agent_id="exploit_gpt",
            memory_strategy=analysis_memory,
        )
        coordinator.agents["exploit_tester"].memory = AsyncMock()
        coordinator.agents["exploit_tester"].memory.add_item = AsyncMock(
            return_value=True
        )
        coordinator.agents["exploit_tester"].memory.search = AsyncMock(return_value=[])
        coordinator.agents["exploit_tester"].model_router = model_router

        coordinator.agents["reporter"] = ReportGenerator(
            agent_id="reporter_gpt",
            memory_strategy=recon_memory,
        )
        coordinator.agents["reporter"].memory = AsyncMock()
        coordinator.agents["reporter"].memory.add_item = AsyncMock(return_value=True)
        coordinator.agents["reporter"].memory.search = AsyncMock(return_value=[])
        coordinator.agents["reporter"].model_router = model_router

        print(f"✓ Initialized {len(coordinator.agents)} agents with GPT")

    # Use our custom setup
    await setup_agents_with_gpt()

    print("\n" + "=" * 60)
    print("RUNNING GPT-POWERED PENETRATION TEST")
    print("=" * 60 + "\n")

    # Run the penetration test on a target
    target = "example-webapp.com"
    print(f"Target: {target}\n")

    try:
        results = await coordinator.run_pentest(target)

        # Display results
        if "report" in results:
            report = results["report"]["report"]

            print("\n" + "=" * 60)
            print("GPT-GENERATED SECURITY ASSESSMENT REPORT")
            print("=" * 60 + "\n")

            print("EXECUTIVE SUMMARY:")
            print("-" * 40)
            print(report.get("executive_summary", "No summary available"))

            print("\n\nTARGET INFORMATION:")
            print("-" * 40)
            print(f"Target: {report.get('target', 'Unknown')}")
            print(f"Assessment Date: {report.get('assessment_date', 'Unknown')}")
            print(f"Overall Risk: {report.get('overall_risk', 'Unknown')}")

            print("\n\nFINDINGS:")
            print("-" * 40)
            findings = report.get("findings", {})
            for key, value in findings.items():
                print(f"  • {key.replace('_', ' ').title()}: {value}")

            print("\n\nRECOMMENDATIONS:")
            print("-" * 40)
            recommendations = report.get("recommendations", {})

            if "immediate" in recommendations:
                print("\nImmediate Actions:")
                for rec in recommendations["immediate"][:5]:
                    print(f"  ✓ {rec}")

            if "short_term" in recommendations:
                print("\nShort-term Actions:")
                for rec in recommendations["short_term"][:3]:
                    print(f"  → {rec}")

            if "long_term" in recommendations:
                print("\nLong-term Strategy:")
                for rec in recommendations["long_term"][:3]:
                    print(f"  ⟶ {rec}")

            print("\n\nPERFORMANCE METRICS:")
            print("-" * 40)
            if "performance" in results:
                perf = results["performance"]
                print(f"  Total Time: {perf.get('total_time', 0):.2f} seconds")
                print(f"  Recon Time: {perf.get('recon_time', 0):.2f} seconds")
                print(f"  Analysis Time: {perf.get('vuln_time', 0):.2f} seconds")
                print(f"  Testing Time: {perf.get('exploit_time', 0):.2f} seconds")
                print(f"  Report Time: {perf.get('report_time', 0):.2f} seconds")
                print("  Model Used: GPT-4-turbo-preview")
                print(f"  Agents Used: {perf.get('agents_used', 0)}")

            # Show model analysis examples
            print("\n\nGPT MODEL INSIGHTS:")
            print("-" * 40)

            # Show reconnaissance model analysis if available
            if "recon" in results:
                recon_data = results.get("recon", {})
                if isinstance(recon_data, dict):
                    # The result_id and summary are in the recon result
                    print("\n[Reconnaissance Phase - GPT Analysis]")
                    print(
                        "GPT helped identify target characteristics and guided the reconnaissance strategy."
                    )

            # Show vulnerability model analysis
            if "vulnerabilities" in results:
                vuln_data = results.get("vulnerabilities", {})
                if isinstance(vuln_data, dict) and "summary" in vuln_data:
                    summary = vuln_data["summary"]
                    print("\n[Vulnerability Analysis - GPT Enhanced]")
                    print(
                        f"Found {summary.get('vulnerabilities_found', 0)} vulnerabilities"
                    )
                    print(f"Risk Level: {summary.get('risk_level', 'Unknown')}")
                    print(f"Risk Score: {summary.get('risk_score', 0)}")
                    print(
                        f"Model-Enhanced Analysis: {summary.get('model_enhanced', False)}"
                    )

            print("\n" + "=" * 60)
            print("GPT-POWERED PENETRATION TEST COMPLETE")
            print("=" * 60)

            print("\n✅ Successfully demonstrated:")
            print("  1. GPT model integration for intelligent analysis")
            print("  2. Multi-agent coordination with ResultPublisher")
            print("  3. Secure result sharing between agents")
            print("  4. Complete workflow from recon to report")
            print("  5. Real LLM-enhanced security assessment")

        else:
            print("❌ Test did not complete successfully")
            print(f"Results: {results}")

    except Exception as e:
        print(f"\n❌ Error during test: {e}")
        import traceback

        traceback.print_exc()


if __name__ == "__main__":
    # Check for API key
    if not os.getenv("OPENAI_API_KEY"):
        print("❌ OPENAI_API_KEY environment variable not set")
        print("Please set: export OPENAI_API_KEY='your-key'")
        exit(1)

    print(f"✓ OpenAI API key found: {os.getenv('OPENAI_API_KEY')[:10]}...")

    # Run the test
    asyncio.run(run_with_gpt())
