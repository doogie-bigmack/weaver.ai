"""
Penetration test configured for GPT-5 (when available) with fallback to GPT-4.
This script is future-proof and will work with GPT-5 when OpenAI releases it.
"""

import asyncio
import os
import sys
from unittest.mock import AsyncMock

sys.path.insert(0, ".")

from tests.integration.test_full_pentest_workflow import (
    ExploitTester,
    PenTestCoordinator,
    ReconAgent,
    ReportGenerator,
    VulnerabilityAnalyzer,
)
from weaver_ai.agents.publisher import ResultPublisher
from weaver_ai.memory import MemoryStrategy
from weaver_ai.models import ModelRouter


async def run_with_latest_gpt():
    """Run the penetration test with the latest available GPT model."""

    print("\n" + "=" * 60)
    print("GPT MODEL SELECTION")
    print("=" * 60 + "\n")

    # Model selection - will use GPT-5 when available
    available_models = {
        "gpt-5": "GPT-5 (Future - Not Yet Released)",
        "gpt-4-turbo-preview": "GPT-4 Turbo (Current Best)",
        "gpt-4": "GPT-4 (Stable)",
        "gpt-3.5-turbo": "GPT-3.5 Turbo (Fast & Cheap)",
    }

    # Try models in order of preference
    model_to_use = None
    for model in ["gpt-5", "gpt-4-turbo-preview", "gpt-4"]:
        if model == "gpt-5":
            # Check if GPT-5 is available via environment variable or API check
            # For now, we know it's not released yet
            print("⚠️  GPT-5 not yet available, falling back to GPT-4")
            continue
        else:
            model_to_use = model
            break

    print(f"✓ Selected model: {model_to_use} - {available_models[model_to_use]}")

    # Create mock publisher
    publisher = ResultPublisher()
    publisher.redis = AsyncMock()
    publisher._connected = True

    # Configure mock Redis
    publisher.redis.setex = AsyncMock(return_value=True)
    publisher.redis.sadd = AsyncMock(return_value=1)
    publisher.redis.zadd = AsyncMock(return_value=1)
    publisher.redis.expire = AsyncMock(return_value=True)
    publisher.redis.smembers = AsyncMock(return_value=[])
    publisher.redis.zrevrange = AsyncMock(return_value=[])

    stored_results = {}

    async def mock_get(key):
        return stored_results.get(key)

    async def mock_setex(key, ttl, value):
        stored_results[key] = value
        return True

    publisher.redis.get = AsyncMock(side_effect=mock_get)
    publisher.redis.setex = AsyncMock(side_effect=mock_setex)

    print("\nConfiguring model router...")
    coordinator = PenTestCoordinator(publisher, use_model=True)

    async def setup_agents_with_model():
        model_router = ModelRouter(load_mock=False)

        # Configure for the selected model (GPT-5 ready)
        model_router.add_model(
            name="gpt",
            adapter_type="openai-compatible",
            base_url="https://api.openai.com/v1",
            api_key=os.getenv("OPENAI_API_KEY"),
            model=model_to_use,  # Will use GPT-5 when available
        )

        print(f"✓ Configured {model_to_use} model")
        print("  Note: When GPT-5 is released, simply change model to 'gpt-5'")

        # Memory strategies
        recon_memory = MemoryStrategy(
            short_term_size=100,
            long_term_size=1000,
            short_term_ttl=1800,
            long_term_ttl=86400,
        )

        analysis_memory = MemoryStrategy(
            short_term_size=200,
            long_term_size=2000,
            short_term_ttl=3600,
            long_term_ttl=172800,
        )

        # Create agents
        coordinator.agents["recon"] = ReconAgent(
            agent_id="recon_gpt5_ready",
            memory_strategy=recon_memory,
        )
        coordinator.agents["recon"].memory = AsyncMock()
        coordinator.agents["recon"].memory.add_item = AsyncMock(return_value=True)
        coordinator.agents["recon"].memory.search = AsyncMock(return_value=[])
        coordinator.agents["recon"].model_router = model_router

        coordinator.agents["vuln_analyzer"] = VulnerabilityAnalyzer(
            agent_id="vuln_gpt5_ready",
            memory_strategy=analysis_memory,
        )
        coordinator.agents["vuln_analyzer"].memory = AsyncMock()
        coordinator.agents["vuln_analyzer"].memory.add_item = AsyncMock(
            return_value=True
        )
        coordinator.agents["vuln_analyzer"].memory.search = AsyncMock(return_value=[])
        coordinator.agents["vuln_analyzer"].model_router = model_router

        coordinator.agents["exploit_tester"] = ExploitTester(
            agent_id="exploit_gpt5_ready",
            memory_strategy=analysis_memory,
        )
        coordinator.agents["exploit_tester"].memory = AsyncMock()
        coordinator.agents["exploit_tester"].memory.add_item = AsyncMock(
            return_value=True
        )
        coordinator.agents["exploit_tester"].memory.search = AsyncMock(return_value=[])
        coordinator.agents["exploit_tester"].model_router = model_router

        coordinator.agents["reporter"] = ReportGenerator(
            agent_id="reporter_gpt5_ready",
            memory_strategy=recon_memory,
        )
        coordinator.agents["reporter"].memory = AsyncMock()
        coordinator.agents["reporter"].memory.add_item = AsyncMock(return_value=True)
        coordinator.agents["reporter"].memory.search = AsyncMock(return_value=[])
        coordinator.agents["reporter"].model_router = model_router

        print(f"✓ Initialized {len(coordinator.agents)} agents")

    await setup_agents_with_model()

    print("\n" + "=" * 60)
    print(f"RUNNING PENETRATION TEST WITH {model_to_use.upper()}")
    print("=" * 60 + "\n")

    target = "future-webapp.example.com"
    print(f"Target: {target}\n")

    try:
        results = await coordinator.run_pentest(target)

        if "report" in results:
            report = results["report"]["report"]

            print("\n" + "=" * 60)
            print(f"{model_to_use.upper()} SECURITY ASSESSMENT REPORT")
            print("=" * 60 + "\n")

            print("MODEL INFORMATION:")
            print("-" * 40)
            print(f"Model Used: {model_to_use}")
            print(f"Status: {available_models[model_to_use]}")
            if model_to_use != "gpt-5":
                print("Note: GPT-5 will be used automatically when available")

            print("\n\nEXECUTIVE SUMMARY:")
            print("-" * 40)
            print(report.get("executive_summary", "No summary available"))

            print("\n\nFINDINGS:")
            print("-" * 40)
            findings = report.get("findings", {})
            for key, value in findings.items():
                print(f"  • {key.replace('_', ' ').title()}: {value}")

            print("\n\nPERFORMANCE:")
            if "performance" in results:
                perf = results["performance"]
                print(f"  Total Time: {perf.get('total_time', 0):.2f} seconds")
                print(f"  Model: {model_to_use}")

            print("\n" + "=" * 60)
            print("TEST COMPLETE")
            print("=" * 60)

            print("\n✅ Framework ready for GPT-5:")
            print("  • Flexible model routing supports any model")
            print("  • No code changes needed when GPT-5 releases")
            print("  • Simply update model parameter to 'gpt-5'")
            print("  • All agent coordination will work unchanged")

        else:
            print("❌ Test did not complete successfully")

    except Exception as e:
        print(f"\n❌ Error: {e}")
        import traceback

        traceback.print_exc()


if __name__ == "__main__":
    if not os.getenv("OPENAI_API_KEY"):
        print("❌ OPENAI_API_KEY not set")
        print("Please run: export OPENAI_API_KEY='your-key'")
        exit(1)

    print("✓ API key found")
    asyncio.run(run_with_latest_gpt())
